name: kinc CI

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:

permissions:
  contents: read

jobs:
  test-baked-in:
    runs-on: ubuntu-latest
    name: Test (baked-in config)
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: System prerequisites check
        run: |
          echo "=== System Prerequisites Check ==="
          
          # IP forwarding
          echo "━━━ Check 1: IP Forwarding ━━━"
          if [ "$(cat /proc/sys/net/ipv4/ip_forward)" != "1" ]; then
            echo "Enabling IP forwarding..."
            echo 1 | sudo tee /proc/sys/net/ipv4/ip_forward
          fi
          echo "✅ IP forwarding: enabled"
          echo ""
          
          # Inotify limits
          echo "━━━ Check 2: Inotify Limits ━━━"
          echo "  Current max_user_watches:   $(cat /proc/sys/fs/inotify/max_user_watches)"
          echo "  Current max_user_instances: $(cat /proc/sys/fs/inotify/max_user_instances)"
          echo "Setting inotify limits for multi-cluster testing..."
          echo 524288 | sudo tee /proc/sys/fs/inotify/max_user_watches > /dev/null
          echo 2048 | sudo tee /proc/sys/fs/inotify/max_user_instances > /dev/null
          echo "✅ Inotify limits: configured (watches=524288, instances=2048)"
          echo ""
          
          # Kernel keyring limits
          echo "━━━ Check 3: Kernel Keyring Limits ━━━"
          echo "  Current maxkeys:   $(cat /proc/sys/kernel/keys/maxkeys 2>/dev/null || echo 'N/A')"
          echo "  Current maxbytes:  $(cat /proc/sys/kernel/keys/maxbytes 2>/dev/null || echo 'N/A')"
          echo "Setting kernel keyring limits for multi-cluster testing..."
          echo 1000 | sudo tee /proc/sys/kernel/keys/maxkeys > /dev/null
          echo 25000 | sudo tee /proc/sys/kernel/keys/maxbytes > /dev/null
          echo "✅ Kernel keyring limits: configured (maxkeys=1000, maxbytes=25000)"
          echo ""
          
          # Failed services
          echo "━━━ Check 4: System Health ━━━"
          failed=$(systemctl --user list-units --state=failed --no-pager --no-legend 2>/dev/null | wc -l)
          if [ $failed -gt 0 ]; then
            echo "⚠️  Found $failed failed services"
            systemctl --user list-units --state=failed --no-pager
          else
            echo "✅ No failed services"
          fi
          echo ""
          
          # Podman
          echo "━━━ Check 5: Podman ━━━"
          echo "✅ Podman: $(podman --version)"
          echo ""
          
          echo "✅ Prerequisites check complete"
        shell: bash

      - name: Build kinc image
        run: |
          echo "=== Building kinc image ==="
          ./tools/build.sh
        shell: bash

      - name: Deploy cluster (baked-in config)
        run: |
          echo "=== Deploying cluster with baked-in configuration ==="
          
          export CLUSTER_NAME=default
          export USE_BAKED_IN_CONFIG=true
          export KINC_ENABLE_FARO=true
          
          # deploy.sh handles everything:
          # - Prerequisites check
          # - Deployment
          # - Wait for initialization (1500s timeout)
          # - Verify multi-service architecture
          ./tools/deploy.sh
          
          echo ""
          echo "✅ Deployment complete (verified by deploy.sh)"
        shell: bash

      - name: Verify cluster is functional
        run: |
          echo "=== Verifying Cluster Functionality ==="
          echo ""
          
          # Extract kubeconfig
          echo "Extracting kubeconfig..."
          mkdir -p ~/.kube
          cluster_port=$(podman inspect kinc-default-control-plane --format '{{range $p, $conf := .NetworkSettings.Ports}}{{range $conf}}{{.HostPort}}{{end}}{{end}}' 2>/dev/null)
          podman cp kinc-default-control-plane:/etc/kubernetes/admin.conf ~/.kube/config
          sed -i "s|server: https://.*:6443|server: https://127.0.0.1:$cluster_port|g" ~/.kube/config
          echo "✅ Kubeconfig extracted (port: $cluster_port)"
          echo ""
          
          # Verify cluster responds
          echo "Cluster info:"
          kubectl cluster-info
          echo ""
          
          echo "Nodes:"
          kubectl get nodes -o wide
          echo ""
          
          echo "System pods:"
          kubectl get pods -A -o wide
          echo ""
          
          echo "✅ Cluster is functional"
        shell: bash

      - name: Validate storage and networking
        run: |
          echo "=== Validating Storage & Networking ==="
          echo ""
          
          # Install Gateway API CRDs
          echo "Installing Gateway API CRDs..."
          kubectl apply --server-side -f https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.4.1/standard-install.yaml
          echo "✅ Gateway API CRDs installed"
          echo ""
          
          # Deploy test workload
          echo "Deploying test workload (PVC + Pod + Service)..."
          kubectl apply -f runtime/manifests/test-workload.yaml
          echo ""
          
          # Wait for PVC to be bound
          echo "Waiting for PVC to bind..."
          kubectl wait --for=jsonpath='{.status.phase}'=Bound pvc/test-pvc --timeout=60s
          echo "✅ PVC bound"
          echo ""
          
          # Wait for pod to be ready (init container + main container)
          echo "Waiting for pod to be ready (init + http server)..."
          kubectl wait --for=condition=Ready pod/test-pod --timeout=90s
          echo "✅ Pod ready"
          echo ""
          
          # Verify JSON was created on PVC
          echo "Verifying data generation on PVC..."
          if kubectl exec test-pod -c http-server -- test -f /data/runtime-info.json; then
            echo "✅ Runtime info JSON created"
          else
            echo "❌ JSON file not found"
            exit 1
          fi
          echo ""
          
          # Test HTTP service and parse JSON
          echo "Testing HTTP service and JSON parsing..."
          kubectl exec test-pod -c http-server -- cat /data/runtime-info.json > /tmp/runtime-info.json
          
          # Parse with jq
          runtime=$(jq -r '.runtime' /tmp/runtime-info.json)
          version=$(jq -r '.version' /tmp/runtime-info.json)
          k8s_version=$(jq -r '.kubernetes_version' /tmp/runtime-info.json)
          
          echo "✅ JSON parsing successful:"
          echo "  Runtime: $runtime v$version"
          echo "  Kubernetes: $k8s_version"
          echo "  Features: $(jq -r '.features | join(", ")' /tmp/runtime-info.json)"
          echo ""
          
          # Show details
          echo "Workload Status:"
          kubectl get pvc,pod,svc -l app=test-workload -o wide || kubectl get pvc test-pvc && kubectl get pod test-pod && kubectl get svc test-service
          echo ""
          echo "Gateway API Resources:"
          kubectl get gatewayclass,gateway,httproute 2>/dev/null || echo "Gateway API CRDs not installed"
          echo ""
          
          # Cleanup
          echo "Cleaning up test workload..."
          kubectl delete -f runtime/manifests/test-workload.yaml
          rm -f /tmp/runtime-info.json
          echo "✅ Storage & networking validation complete"
        shell: bash

      - name: Collect comprehensive diagnostics
        if: always()
        run: |
          echo "=== Collecting Comprehensive Diagnostics ==="
          mkdir -p artifacts/logs
          
          # Host-side diagnostics
          echo "Collecting host-side diagnostics..."
          systemctl --user status kinc-default-control-plane.service --no-pager > artifacts/systemd-status.txt 2>&1 || true
          journalctl --user -xeu kinc-default-control-plane.service --no-pager > artifacts/systemd-logs.txt 2>&1 || true
          
          # Container-side multi-service logs (from boot)
          echo "Collecting multi-service logs from boot..."
          podman exec kinc-default-control-plane journalctl -u kinc-preflight.service --no-pager --boot > artifacts/logs/kinc-preflight.log 2>&1 || true
          podman exec kinc-default-control-plane journalctl -u kubeadm-init.service --no-pager --boot > artifacts/logs/kubeadm-init.log 2>&1 || true
          podman exec kinc-default-control-plane journalctl -u kinc-postinit.service --no-pager --boot > artifacts/logs/kinc-postinit.log 2>&1 || true
          podman exec kinc-default-control-plane journalctl -u crio.service --no-pager --boot > artifacts/logs/crio.log 2>&1 || true
          podman exec kinc-default-control-plane journalctl -u kubelet.service --no-pager --boot > artifacts/logs/kubelet.log 2>&1 || true
          podman exec kinc-default-control-plane journalctl --no-pager --boot > artifacts/logs/system-full.log 2>&1 || true
          
          # Service status summary
          echo "Collecting service status summary..."
          {
            echo "=== Multi-Service Architecture Status ==="
            echo ""
            echo "━━━ kinc-preflight.service ━━━"
            podman exec kinc-default-control-plane systemctl status kinc-preflight.service --no-pager 2>&1 || true
            echo ""
            echo "━━━ kubeadm-init.service ━━━"
            podman exec kinc-default-control-plane systemctl status kubeadm-init.service --no-pager 2>&1 || true
            echo ""
            echo "━━━ kinc-postinit.service ━━━"
            podman exec kinc-default-control-plane systemctl status kinc-postinit.service --no-pager 2>&1 || true
          } > artifacts/service-status.txt
          
          # Kubernetes diagnostics
          echo "Collecting Kubernetes diagnostics..."
          kubectl cluster-info dump > artifacts/cluster-info.txt 2>&1 || true
          kubectl get all -A > artifacts/resources.txt 2>&1 || true
          kubectl get events -A --sort-by='.lastTimestamp' > artifacts/events.txt 2>&1 || true
          
          # Network diagnostics
          echo "Collecting network diagnostics..."
          podman ps --all > artifacts/containers.txt 2>&1 || true
          podman port kinc-default-control-plane > artifacts/port-mapping.txt 2>&1 || true
          podman inspect kinc-default-control-plane > artifacts/container-inspect.json 2>&1 || true
          
          echo "✅ Diagnostics collection complete"
          echo ""
          echo "Collected files:"
          ls -lh artifacts/
          ls -lh artifacts/logs/
        shell: bash

      - name: Upload diagnostics
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: kinc-diagnostics-baked-in-${{ github.run_number }}
          path: artifacts/
          retention-days: 30

      - name: Extract Faro events
        if: always()
        run: |
          echo "=== Extracting Faro Events ==="
          
          # Extract directly from data volume (no podman exec needed)
          FARO_PATH="$HOME/.local/share/containers/storage/volumes/kinc-default-var-data/_data/lib/kinc/faro-events/logs"
          
          if [ -d "$FARO_PATH" ] && [ -n "$(ls -A $FARO_PATH/*.json 2>/dev/null)" ]; then
            echo "✅ Faro events found in data volume"
            
            mkdir -p artifacts/faro
            
            # Copy events from volume
            cp $FARO_PATH/*.json artifacts/faro/ 2>/dev/null || true
            cp $FARO_PATH/*.log artifacts/faro/ 2>/dev/null || true
            
            # Count events
            EVENT_COUNT=$(cat artifacts/faro/*.json 2>/dev/null | wc -l)
            echo "✅ Captured $EVENT_COUNT events"
            
            # Show summary
            echo ""
            echo "Event Summary:"
            cat artifacts/faro/*.json | jq -r '.gvr' | sort | uniq -c | sort -rn | head -10
          else
            echo "⚠️  Faro events not found - skipping extraction"
          fi
        shell: bash

      - name: Upload Faro events
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: faro-events-baked-in-${{ github.run_number }}
          path: artifacts/faro/
          retention-days: 30

      - name: Cleanup
        if: always()
        run: |
          echo "=== Cleaning up ==="
          ./tools/cleanup.sh default || true
          systemctl --user reset-failed || true
          echo "✅ Cleanup complete"
        shell: bash

  test-mounted:
    runs-on: ubuntu-latest
    name: Test (mounted config)
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: System prerequisites check
        run: |
          echo "=== System Prerequisites Check ==="
          
          # IP forwarding
          echo "━━━ Check 1: IP Forwarding ━━━"
          if [ "$(cat /proc/sys/net/ipv4/ip_forward)" != "1" ]; then
            echo "Enabling IP forwarding..."
            echo 1 | sudo tee /proc/sys/net/ipv4/ip_forward
          fi
          echo "✅ IP forwarding: enabled"
          echo ""
          
          # Inotify limits
          echo "━━━ Check 2: Inotify Limits ━━━"
          echo "  Current max_user_watches:   $(cat /proc/sys/fs/inotify/max_user_watches)"
          echo "  Current max_user_instances: $(cat /proc/sys/fs/inotify/max_user_instances)"
          echo "Setting inotify limits for multi-cluster testing..."
          echo 524288 | sudo tee /proc/sys/fs/inotify/max_user_watches > /dev/null
          echo 2048 | sudo tee /proc/sys/fs/inotify/max_user_instances > /dev/null
          echo "✅ Inotify limits: configured (watches=524288, instances=2048)"
          echo ""
          
          # Kernel keyring limits
          echo "━━━ Check 3: Kernel Keyring Limits ━━━"
          echo "  Current maxkeys:   $(cat /proc/sys/kernel/keys/maxkeys 2>/dev/null || echo 'N/A')"
          echo "  Current maxbytes:  $(cat /proc/sys/kernel/keys/maxbytes 2>/dev/null || echo 'N/A')"
          echo "Setting kernel keyring limits for multi-cluster testing..."
          echo 1000 | sudo tee /proc/sys/kernel/keys/maxkeys > /dev/null
          echo 25000 | sudo tee /proc/sys/kernel/keys/maxbytes > /dev/null
          echo "✅ Kernel keyring limits: configured (maxkeys=1000, maxbytes=25000)"
          echo ""
          
          # Failed services
          echo "━━━ Check 4: System Health ━━━"
          failed=$(systemctl --user list-units --state=failed --no-pager --no-legend 2>/dev/null | wc -l)
          if [ $failed -gt 0 ]; then
            echo "⚠️  Found $failed failed services"
            systemctl --user list-units --state=failed --no-pager
          else
            echo "✅ No failed services"
          fi
          echo ""
          
          # Podman
          echo "━━━ Check 5: Podman ━━━"
          echo "✅ Podman: $(podman --version)"
          echo ""
          
          echo "✅ Prerequisites check complete"
        shell: bash

      - name: Build kinc image
        run: |
          echo "=== Building kinc image ==="
          ./tools/build.sh
        shell: bash

      - name: Deploy clusters (mounted config)
        run: |
          echo "=== Deploying 2 clusters with mounted configuration ==="
          echo "This demonstrates multi-cluster capability with mounted configs"
          echo ""
          
          # Deploy cluster 1
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Deploying cluster: default"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          export CLUSTER_NAME=default
          export KINC_ENABLE_FARO=true
          # deploy.sh handles: prerequisites, deployment, wait, verification
          ./tools/deploy.sh
          echo ""
          
          # Deploy cluster 2
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Deploying cluster: cluster01"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          export CLUSTER_NAME=cluster01
          export KINC_ENABLE_FARO=true
          # deploy.sh handles: prerequisites, deployment, wait, verification
          ./tools/deploy.sh
          echo ""
          
          echo "✅ Both clusters deployed and verified (by deploy.sh)"
        shell: bash

      - name: Verify clusters are functional
        run: |
          echo "=== Verifying Both Clusters Functionality ==="
          mkdir -p ~/.kube
          
          for cluster in default cluster01; do
            echo ""
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "Cluster: $cluster"
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            
            # Extract kubeconfig
            cluster_port=$(podman inspect kinc-${cluster}-control-plane --format '{{range $p, $conf := .NetworkSettings.Ports}}{{range $conf}}{{.HostPort}}{{end}}{{end}}' 2>/dev/null)
            podman cp kinc-${cluster}-control-plane:/etc/kubernetes/admin.conf ~/.kube/kinc-${cluster}-config
            sed -i "s|server: https://.*:6443|server: https://127.0.0.1:$cluster_port|g" ~/.kube/kinc-${cluster}-config
            echo "✅ Kubeconfig extracted (port: $cluster_port)"
            echo ""
            
            export KUBECONFIG=~/.kube/kinc-${cluster}-config
            
            # Verify cluster responds
            echo "Cluster info:"
            kubectl cluster-info
            echo ""
            
            echo "Nodes:"
            kubectl get nodes -o wide
            echo ""
            
            echo "System pods:"
            kubectl get pods -A -o wide
            echo ""
            
            echo "✅ Cluster '$cluster' is functional"
          done
          
          echo ""
          echo "✅ Both clusters are functional"
        shell: bash

      - name: Validate storage and networking
        run: |
          echo "=== Validating Storage & Networking (default cluster) ==="
          echo ""
          
          export KUBECONFIG=~/.kube/kinc-default-config
          
          # Install Gateway API CRDs
          echo "Installing Gateway API CRDs..."
          kubectl apply --server-side -f https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.4.1/standard-install.yaml
          echo "✅ Gateway API CRDs installed"
          echo ""
          
          # Deploy test workload
          echo "Deploying test workload (PVC + Pod + Service)..."
          kubectl apply -f runtime/manifests/test-workload.yaml
          echo ""
          
          # Wait for PVC to be bound
          echo "Waiting for PVC to bind..."
          kubectl wait --for=jsonpath='{.status.phase}'=Bound pvc/test-pvc --timeout=60s
          echo "✅ PVC bound"
          echo ""
          
          # Wait for pod to be ready (init container + main container)
          echo "Waiting for pod to be ready (init + http server)..."
          kubectl wait --for=condition=Ready pod/test-pod --timeout=90s
          echo "✅ Pod ready"
          echo ""
          
          # Verify JSON was created on PVC
          echo "Verifying data generation on PVC..."
          if kubectl exec test-pod -c http-server -- test -f /data/runtime-info.json; then
            echo "✅ Runtime info JSON created"
          else
            echo "❌ JSON file not found"
            exit 1
          fi
          echo ""
          
          # Test HTTP service and parse JSON
          echo "Testing HTTP service and JSON parsing..."
          kubectl exec test-pod -c http-server -- cat /data/runtime-info.json > /tmp/runtime-info.json
          
          # Parse with jq
          runtime=$(jq -r '.runtime' /tmp/runtime-info.json)
          version=$(jq -r '.version' /tmp/runtime-info.json)
          k8s_version=$(jq -r '.kubernetes_version' /tmp/runtime-info.json)
          
          echo "✅ JSON parsing successful:"
          echo "  Runtime: $runtime v$version"
          echo "  Kubernetes: $k8s_version"
          echo "  Features: $(jq -r '.features | join(", ")' /tmp/runtime-info.json)"
          echo ""
          
          # Show details
          echo "Workload Status:"
          kubectl get pvc,pod,svc -l app=test-workload -o wide || kubectl get pvc test-pvc && kubectl get pod test-pod && kubectl get svc test-service
          echo ""
          echo "Gateway API Resources:"
          kubectl get gatewayclass,gateway,httproute 2>/dev/null || echo "Gateway API CRDs not installed"
          echo ""
          
          # Cleanup
          echo "Cleaning up test workload..."
          kubectl delete -f runtime/manifests/test-workload.yaml
          rm -f /tmp/runtime-info.json
          echo "✅ Storage & networking validation complete"
        shell: bash

      - name: Collect comprehensive diagnostics
        if: always()
        run: |
          echo "=== Collecting Comprehensive Diagnostics (Both Clusters) ==="
          mkdir -p artifacts/logs
          
          for cluster in default cluster01; do
            echo "Collecting diagnostics for cluster: $cluster"
            mkdir -p artifacts/${cluster}/logs
            
            # Host-side diagnostics
            echo "Collecting host-side diagnostics..."
            systemctl --user status kinc-${cluster}-control-plane.service --no-pager > artifacts/${cluster}/systemd-status.txt 2>&1 || true
            journalctl --user -xeu kinc-${cluster}-control-plane.service --no-pager > artifacts/${cluster}/systemd-logs.txt 2>&1 || true
            
            # Container-side multi-service logs (from boot)
            echo "Collecting multi-service logs from boot..."
            podman exec kinc-${cluster}-control-plane journalctl -u kinc-preflight.service --no-pager --boot > artifacts/${cluster}/logs/kinc-preflight.log 2>&1 || true
            podman exec kinc-${cluster}-control-plane journalctl -u kubeadm-init.service --no-pager --boot > artifacts/${cluster}/logs/kubeadm-init.log 2>&1 || true
            podman exec kinc-${cluster}-control-plane journalctl -u kinc-postinit.service --no-pager --boot > artifacts/${cluster}/logs/kinc-postinit.log 2>&1 || true
            podman exec kinc-${cluster}-control-plane journalctl -u crio.service --no-pager --boot > artifacts/${cluster}/logs/crio.log 2>&1 || true
            podman exec kinc-${cluster}-control-plane journalctl -u kubelet.service --no-pager --boot > artifacts/${cluster}/logs/kubelet.log 2>&1 || true
            podman exec kinc-${cluster}-control-plane journalctl --no-pager --boot > artifacts/${cluster}/logs/system-full.log 2>&1 || true
            
            # Service status summary
            echo "Collecting service status summary..."
            {
              echo "=== Multi-Service Architecture Status: $cluster ==="
              echo ""
              echo "━━━ kinc-preflight.service ━━━"
              podman exec kinc-${cluster}-control-plane systemctl status kinc-preflight.service --no-pager 2>&1 || true
              echo ""
              echo "━━━ kubeadm-init.service ━━━"
              podman exec kinc-${cluster}-control-plane systemctl status kubeadm-init.service --no-pager 2>&1 || true
              echo ""
              echo "━━━ kinc-postinit.service ━━━"
              podman exec kinc-${cluster}-control-plane systemctl status kinc-postinit.service --no-pager 2>&1 || true
            } > artifacts/${cluster}/service-status.txt
            
            # Kubernetes diagnostics
            echo "Collecting Kubernetes diagnostics..."
            export KUBECONFIG=~/.kube/kinc-${cluster}-config
            kubectl cluster-info dump > artifacts/${cluster}/cluster-info.txt 2>&1 || true
            kubectl get all -A > artifacts/${cluster}/resources.txt 2>&1 || true
            kubectl get events -A --sort-by='.lastTimestamp' > artifacts/${cluster}/events.txt 2>&1 || true
            
            # Network diagnostics
            echo "Collecting network diagnostics..."
            podman port kinc-${cluster}-control-plane > artifacts/${cluster}/port-mapping.txt 2>&1 || true
            podman inspect kinc-${cluster}-control-plane > artifacts/${cluster}/container-inspect.json 2>&1 || true
            
            echo "✅ Diagnostics collected for cluster: $cluster"
          done
          
          # Collect podman ps for all clusters
          podman ps --all > artifacts/containers.txt 2>&1 || true
          
          echo "✅ Diagnostics collection complete"
          echo ""
          echo "Collected files:"
          find artifacts/ -type f
        shell: bash

      - name: Upload diagnostics
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: kinc-diagnostics-mounted-${{ github.run_number }}
          path: artifacts/
          retention-days: 30

      - name: Extract Faro events from both clusters
        if: always()
        run: |
          echo "=== Extracting Faro Events from Both Clusters ==="
          
          mkdir -p artifacts/faro
          
          for cluster in default cluster01; do
            echo ""
            echo "Extracting events from cluster: $cluster"
            
            # Extract directly from data volume (no podman exec needed)
            FARO_PATH="$HOME/.local/share/containers/storage/volumes/kinc-${cluster}-var-data/_data/lib/kinc/faro-events/logs"
            
            if [ -d "$FARO_PATH" ] && [ -n "$(ls -A $FARO_PATH/*.json 2>/dev/null)" ]; then
              echo "✅ Faro events found in $cluster data volume"
              
              # Copy events from volume
              cp $FARO_PATH/*.json artifacts/faro/bootstrap-events-${cluster}.json 2>/dev/null || true
              cp $FARO_PATH/*.log artifacts/faro/bootstrap-logs-${cluster}.log 2>/dev/null || true
              
              # Count events
              EVENT_COUNT=$(cat artifacts/faro/bootstrap-events-${cluster}.json 2>/dev/null | wc -l)
              echo "✅ Captured $EVENT_COUNT events from $cluster"
              
              # Show summary
              echo "Event Summary for $cluster:"
              cat artifacts/faro/bootstrap-events-${cluster}.json | jq -r '.gvr' | sort | uniq -c | sort -rn | head -5
            else
              echo "⚠️  Faro events not found in $cluster - skipping extraction"
            fi
          done
          
          echo ""
          echo "✅ Faro event extraction complete for all clusters"
        shell: bash

      - name: Upload Faro events
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: faro-events-mounted-${{ github.run_number }}
          path: artifacts/faro/
          retention-days: 30

      - name: Cleanup
        if: always()
        run: |
          echo "=== Cleaning up both clusters ==="
          ./tools/cleanup.sh default || true
          ./tools/cleanup.sh cluster01 || true
          systemctl --user reset-failed || true
          echo "✅ Cleanup complete"
        shell: bash
